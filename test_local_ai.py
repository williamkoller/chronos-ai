#!/usr/bin/env python3
"""
ğŸ¤– TESTE DE IA LOCAL - Chronos AI
Testa conectividade e funcionalidade do Ollama
"""

import os
import sys
import requests
import json
from datetime import datetime

# Carrega variÃ¡veis de ambiente
from dotenv import load_dotenv
load_dotenv()

def test_ollama_connection():
    """Testa conexÃ£o bÃ¡sica com Ollama"""
    ollama_url = os.getenv('OLLAMA_URL', 'http://localhost:11434')
    
    print("ğŸ¤– TESTE DE IA LOCAL - Chronos AI")
    print("=" * 50)
    print(f"ğŸ”— URL Ollama: {ollama_url}")
    
    # 1. Verifica se Ollama estÃ¡ rodando
    print("\nğŸ” Verificando status do Ollama...")
    try:
        response = requests.get(f"{ollama_url}/api/tags", timeout=5)
        if response.status_code == 200:
            print("âœ… Ollama estÃ¡ rodando!")
            models = response.json().get('models', [])
            print(f"ğŸ“‹ Modelos disponÃ­veis: {len(models)}")
            for model in models:
                print(f"   â€¢ {model['name']}")
            return models
        else:
            print(f"âŒ Ollama respondeu com erro: {response.status_code}")
            return []
    except Exception as e:
        print(f"âŒ Ollama nÃ£o estÃ¡ rodando: {e}")
        print("ğŸ’¡ Execute: docker-compose up -d ollama")
        return []

def test_model_generation(model_name="llama3.2:3b"):
    """Testa geraÃ§Ã£o de texto com modelo"""
    ollama_url = os.getenv('OLLAMA_URL', 'http://localhost:11434')
    
    print(f"\nğŸ§ª Testando geraÃ§Ã£o com modelo: {model_name}")
    
    payload = {
        "model": model_name,
        "prompt": "VocÃª Ã© o Chronos AI, um assistente de agendamento. Responda apenas: OlÃ¡! Estou funcionando perfeitamente em modo local.",
        "stream": False,
        "options": {
            "temperature": 0.7,
            "top_p": 0.9
        }
    }
    
    try:
        response = requests.post(
            f"{ollama_url}/api/generate",
            json=payload,
            timeout=30
        )
        
        if response.status_code == 200:
            result = response.json()
            ai_response = result.get('response', '').strip()
            
            print(f"âœ… Resposta da IA:")
            print(f"   {ai_response}")
            
            # Verifica tempo de resposta
            eval_duration = result.get('eval_duration', 0)
            if eval_duration > 0:
                eval_seconds = eval_duration / 1e9  # nanosegundos para segundos
                print(f"â±ï¸ Tempo de resposta: {eval_seconds:.2f}s")
            
            return True
        else:
            print(f"âŒ Erro na geraÃ§Ã£o: {response.text}")
            return False
            
    except Exception as e:
        print(f"âŒ Erro na geraÃ§Ã£o: {e}")
        return False

def test_chronos_integration():
    """Testa integraÃ§Ã£o com ClaudeClient"""
    print(f"\nğŸ”§ Testando integraÃ§Ã£o Chronos AI...")
    
    # Configura variÃ¡veis de ambiente para IA local
    os.environ['OLLAMA_URL'] = os.getenv('OLLAMA_URL', 'http://localhost:11434')
    os.environ['OLLAMA_MODEL'] = os.getenv('OLLAMA_MODEL', 'llama3.2:3b')
    
    try:
        # Importa cliente
        sys.path.append('.')
        from integrations.ai_client import AIClient
        
        # Cria cliente IA local
        client = AIClient()  # NÃ£o precisa de API key
        
        # Dados de teste
        task_data = {
            "title": "Revisar cÃ³digo Python",
            "category": "Development",
            "estimated_time": 60
        }
        
        user_patterns = {
            "peak_hours": ["09:00-11:00"],
            "productivity_score": 0.8
        }
        
        context = {
            "current_time": datetime.now().isoformat(),
            "workload": "medium"
        }
        
        # Testa geraÃ§Ã£o
        print("ğŸš€ Gerando sugestÃ£o de agendamento...")
        suggestion = client.generate_schedule_suggestion(task_data, user_patterns, context)
        
        if suggestion:
            print("âœ… SugestÃ£o gerada com sucesso!")
            print(f"   ğŸ“… Agendamento: {suggestion.get('scheduled_datetime', 'N/A')}")
            print(f"   ğŸ¯ ConfianÃ§a: {suggestion.get('confidence_score', 'N/A')}")
            print(f"   ğŸ’­ Reasoning: {suggestion.get('reasoning', 'N/A')[:100]}...")
            return True
        else:
            print("âŒ Falha na geraÃ§Ã£o de sugestÃ£o")
            return False
            
    except Exception as e:
        print(f"âŒ Erro na integraÃ§Ã£o: {e}")
        return False

def main():
    # 1. Testa conexÃ£o
    models = test_ollama_connection()
    if not models:
        return 1
    
    # 2. Verifica se tem modelo padrÃ£o
    model_names = [m['name'] for m in models]
    if 'llama3.2:3b' in model_names:
        test_model = 'llama3.2:3b'
    elif model_names:
        test_model = model_names[0]
    else:
        print("âŒ Nenhum modelo disponÃ­vel!")
        print("ğŸ’¡ Execute: python setup_ollama_models.py")
        return 1
    
    # 3. Testa geraÃ§Ã£o
    if not test_model_generation(test_model):
        return 1
    
    # 4. Testa integraÃ§Ã£o
    if not test_chronos_integration():
        return 1
    
    print(f"\nğŸ‰ TODOS OS TESTES PASSARAM!")
    print(f"âœ¨ IA Local configurada e funcionando!")
    print(f"ğŸ“š Modelo ativo: {test_model}")
    print(f"ğŸš€ Execute: docker-compose up -d para usar!")
    
    return 0

if __name__ == "__main__":
    sys.exit(main()) 